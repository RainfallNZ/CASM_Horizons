---
title: "R Notebook to preprocess water quality data for use with CASM, for Horizons"
output: html_notebook
---

Load libraries
```{r}
if (!require(rgdal)) install.packages("rgdal"); library(rgdal)   
if (!require(raster)) install.packages("raster"); library(raster)
if (!require(plyr)) install.packages("plyr"); library(plyr)
if (!require(openxlsx)) install.packages("openxlsx"); library(openxlsx)                #needed to load excel data
if (!require(devtools)) install.packages("devtools"); library(devtools)                #Needed to load libraries from github
if (!require(CASMPrePostProcessor)) install_github("rainfallnz/CASMPrePostProcessor",force=TRUE); library(CASMPrePostProcessor)                #Needed to load libraries from github
```

Some functions
```{r}


#Create scenario leach rate rasters from Horizon's Plan change 2 Table 14.2, from the raster of the Sub-management Zone-Land Use-LUC (and the look up table which #converts the class numbers to descriptions), and the "current" MPI-based leach rate raster

ScenarioLeachRateRaster <- function(CurrentLeachRateRaster = LeachRate, LUC_To_Leach_Table = LeachRateLimits, ZLLRaster = ZoneLanduseLUCRaster, ZoneLanduseLUC_LUTFile = ZoneLanduseLUCCode_To_ClassLUTFile,
                                     plan = "PC2", year = 20,
                                    Intensive = FALSE,
                                    MPIPredictorRasterFile = PredictorRasterFile) {
  #Load the ZoneLanduseLUC raster and the look up table that relates the raster's codes to a combined-class name
  ZoneLanduseLUC_LUT <- read.table(ZoneLanduseLUC_LUTFile,sep=",",header=TRUE)
  
  #Parse the Combined name into its constituent parts
  ZoneLanduseLUC_LUT$Zone <- as.factor(sub('-.*','',ZoneLanduseLUC_LUT$CombinedClassName))
  ZoneLanduseLUC_LUT$ZoneNo <- as.numeric(ZoneLanduseLUC_LUT$Zone)
  ZoneLanduseLUC_LUT$Landuse <- as.factor(sub('.*-(.*)-.*','\\1',ZoneLanduseLUC_LUT$CombinedClassName))
  ZoneLanduseLUC_LUT$LanduseNo <- as.numeric(ZoneLanduseLUC_LUT$Landuse)                                         
  ZoneLanduseLUC_LUT$LUC <- as.numeric(sub('.*-','',ZoneLanduseLUC_LUT$CombinedClassName))

  #reclasify ZoneLanduSeLUCRaster based on levels to the constituent parts
  LandUseRaster <- reclassify(ZLLRaster,as.matrix(ZoneLanduseLUC_LUT[,c('ID','LanduseNo')]))
  SMZRaster <- reclassify(ZLLRaster,as.matrix(ZoneLanduseLUC_LUT[,c('ID','ZoneNo')]))
  LUCRaster <- reclassify(ZLLRaster,as.matrix(ZoneLanduseLUC_LUT[,c('ID','LUC')]))
  
  
  
  #Reclasify to Table 14.2 leach rates as though they applied everywhere
  LeachRateLUT <- matrix(ncol=2,c(1:8,t(LUC_To_Leach_Table[LUC_To_Leach_Table$Year == year & LUC_To_Leach_Table$Plan == plan,c(3:10)])))
  AreaWideScenarioLeachRates <- reclassify(LUCRaster, LeachRateLUT)

  if (Intensive) {
    #Load the MPI Leach rate predictor raster brick which contains the irrigated area data, needed to determine what sheep and beef land should be classed as intensive.
  IrrigatedRaster <- stack(MPIPredictorRasterFile)[[5]]
      #Turn the ZoneLanduseLUCRaster into an "intensive farming" mask, based on rules as to what constitutes intensive farming
  IntensiveFarmingMask <- (LandUseRaster %in% which(levels(ZoneLanduseLUC_LUT$Landuse) %in% c("Dairy","Cropping","Horticulture-Veg","Horticulture-Other"))) |
    (LandUseRaster %in% which(levels(ZoneLanduseLUC_LUT$Landuse) %in% c("Sheep and/or Beef")) & IrrigatedRaster)
  
  #For the intensive farming areas find the Scenario leach rates, otherwise, keep using the MPI leach rates
  ScenarioLeachRate <- AreaWideScenarioLeachRates * IntensiveFarmingMask + CurrentLeachRateRaster * !IntensiveFarmingMask
  
    #Export a raster stack for exploration in QGIS
  ScenarioRasters <- raster::stack(x=list(Scenario=ScenarioLeachRate,ScenarioAll=AreaWideScenarioLeachRates,Mask=IntensiveFarmingMask,MPI=CurrentLeachRateRaster,LUC=LUCRaster,Landuse=LandUseRaster,Irrigation=IrrigatedRaster))
  } else {
    ScenarioLeachRate <- AreaWideScenarioLeachRates
    #Export a raster stack for exploration in QGIS without the intensive landuse mask or the irrigation raster
  ScenarioRasters <- raster::stack(x=list(Scenario=ScenarioLeachRate,ScenarioAll=AreaWideScenarioLeachRates,MPI=CurrentLeachRateRaster,LUC=LUCRaster,Landuse=LandUseRaster))
  }
  
  

  writeRaster(ScenarioRasters,file.path(GISDataDirectory,"ScenarioLeachRasters.tif"),overwrite=TRUE)
  
  
  return(ScenarioLeachRate)
}



#Function to find the total area of each Climate-Plant Available Water class within the "target" Dairy-Land Use Capability areas, and all the irrigated sheep and beef - land use capability combinations.

TargetZoneAreaFinder <- function(ClimateShapeFile=ClimateShapeFile,
                                 SuZoneLandUseLUCShapeFile=SubZoneLanduseLUCShapeFile,
                                 IrrigatedShapeFile=IrrigatedShapeFile,
                                 PAWShapeFile=PAWShapeFile,
                                 TargetSubZonesFile=TargetSubZonesFile){
#Load a bonus library
  if (!require(data.table)) install.packages("data.table"); library(data.table)
#Load climate spatial data
ClimateSpatial <- readOGR(dsn=dirname(ClimateShapeFile),layer = basename(ClimateShapeFile), stringsAsFactors = FALSE)
#Make sure the class id is numeric
ClimateSpatial@data$id <- as.numeric(ClimateSpatial@data$id)
ClimateSpatial <- spTransform(ClimateSpatial,CRS("+init=epsg:2193") )

#Load the spatial data with the landuse, land use capability and submanagement zones alltogether
SubZoneLanduseLUCSpatial <- readOGR(dsn=dirname(SubZoneLanduseLUCShapeFile),layer=basename(SubZoneLanduseLUCShapeFile),stringsAsFactors = FALSE)
SubZoneLanduseLUCSpatial <- spTransform(SubZoneLanduseLUCSpatial,CRS("+init=epsg:2193") )

#Load irrigated land spatial data
IrrigatedLandSpatial <- readOGR(dsn=dirname(IrrigatedShapeFile),layer = basename(IrrigatedShapeFile),stringsAsFactors = FALSE)
IrrigatedLandSpatial@data$Irrigated <- as.numeric(IrrigatedLandSpatial@data$Irrigated)
IrrigatedLandSpatial <- spTransform(IrrigatedLandSpatial,CRS("+init=epsg:2193") )

#Load Plant Available Water (PAW) spatial data
PAWSpatial <- readOGR(dsn= dirname(PAWShapeFile),layer=basename(PAWShapeFile), stringsAsFactors = FALSE)
#Convert PAW values to numbers (I don't know why they are characters)
PAWSpatial$PAW_Cat2 <- as.numeric(PAWSpatial$PAW_Cat2)
PAWSpatial <- spTransform(PAWSpatial,CRS("+init=epsg:2193") )

TargetZones <- readOGR(dsn=dirname(TargetSubZonesFile),layer=basename(TargetSubZonesFile), stringsAsFactors = FALSE)
TargetZones <- spTransform(TargetZones,CRS("+init=epsg:2193") )

#Select the target zones from the Land use data
TargetZoneLanduseLUC <- SubZoneLanduseLUCSpatial[which(SubZoneLanduseLUCSpatial$Zone_Code %in% TargetZones$Zone_Code),]

#only interested in Dairy
TargetZoneLanduseLUC_Dairy<- TargetZoneLanduseLUC[which(TargetZoneLanduseLUC$RegiScLand =="Dairy"),]

#only interested in irrigated sheep and beef, so select the sheep and beef
TargetZoneLanduseLUC_SheepAndBeef <- TargetZoneLanduseLUC[which(TargetZoneLanduseLUC$RegiScLand == "Sheep and/or Beef"),]
#Intersect with irrigated
TargetZoneLanduseLUC_SheepAndBeef_Irrigated <- intersect(TargetZoneLanduseLUC_SheepAndBeef,IrrigatedLandSpatial)
#Select just the Irrigated
TargetZoneLanduseLUC_SheepAndBeef_Irrigated <- TargetZoneLanduseLUC_SheepAndBeef_Irrigated[which(TargetZoneLanduseLUC_SheepAndBeef_Irrigated$Irrigated == 1),]

#Interesect the target zones with Climate
TargetZoneLanduseLUC_Dairy_Climate <- intersect(TargetZoneLanduseLUC_Dairy,ClimateSpatial)

#And then intersect with PAW
TargetZoneLanduseLUC_Dairy_Climate_PAW <- intersect(TargetZoneLanduseLUC_Dairy_Climate,PAWSpatial)
#and add areas
TargetZoneLanduseLUC_Dairy_Climate_PAW$area <- area(TargetZoneLanduseLUC_Dairy_Climate_PAW)
#just get the columns we want
TargetZoneLanduseLUC_Dairy_Climate_PAW <- TargetZoneLanduseLUC_Dairy_Climate_PAW[,c("LUC_CLASS","Zone_Code","Class","id","PAW_Cat2","area")]

#Now sum the areas for the unique combinations of everything.
#This solution is from https://stackoverflow.com/questions/27137174/sum-a-rows-within-a-column-for-each-unique-combination-r

#Do the summing
TargetZoneLanduseLUC_Dairy_Climate_PAW_summedAreas <- setDT(TargetZoneLanduseLUC_Dairy_Climate_PAW@data)[, Sum := round(sum(area)/10000,1), by = .(LUC_CLASS,Zone_Code,Class,PAW_Cat2)][]

#Get the unique values
TargetZoneLanduseLUC_Dairy_Climate_PAW_summedAreas <- unique(TargetZoneLanduseLUC_Dairy_Climate_PAW_summedAreas[,-6])

#Rename
names(TargetZoneLanduseLUC_Dairy_Climate_PAW_summedAreas) <- c("LUC","Zone","Climate","Climate No.","PAW","Area (Ha)")

#Add the landuse
DairyData$Landuse <- "Dairy"


#Repeat for Irrigated Sheep and Beef
TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate<- intersect(TargetZoneLanduseLUC_SheepAndBeef_Irrigated,ClimateSpatial)
TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW <- intersect(TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate,PAWSpatial)
TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW$area <- area(TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW)
TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW <- TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW[,c("LUC_CLASS","Zone_Code","Class","id","PAW_Cat2","area")]


TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW_summedAreas <- setDT(TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW@data)[, Sum := round(sum(area)/10000,1), by = .(LUC_CLASS,Zone_Code,Class,PAW_Cat2)][]

TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW_summedAreas <- unique(TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW_summedAreas[,-6])

names(TargetZoneLanduseLUC_SheepAndBeef_Irrigated_Climate_PAW_summedAreas) <- c("LUC","Zone","Climate","Climate No.","PAW","Area (Ha)")


#Add the landuse
SheepAndBeefIrrigatedData$Landuse <- "SheepAndBeef_Irrigated"

#Combine the Dairy and the SheepAndBeef data
CombinedData <- rbind(DairyData,SheepAndBeefIrrigatedData)
#Reorder the columns
CombinedData <- CombinedData[,c("Zone","Landuse","LUC","Climate","PAW","Area (Ha)")]

#Re-order the rows
CombinedData <- CombinedData[order(CombinedData$Zone, CombinedData$Landuse, CombinedData$LUC, CombinedData$Climate, CombinedData$PAW),]

#Write to a file
#write.table(CombinedData, file.path(DataDirectory,"TargetZone_Dairy_and_Irrigated_Sheep_And_Beef_Categorised_Areas.csv"),sep=",",row.names=FALSE,quote=FALSE)
return(CombinedData)
}


#Function to find the area of each MPI combination, and its N loss
PredictorCombinationAreasAndNLoss <- function(GISDataDirectory=GISDataDirectory,
                                              LeachRateRasterFile=LeachRateRasterFile,
                                              PredictorRasterFile=PredictorRasterFile,
                                              DataDirectory=DataDirectory,
                                              MPILeachRateLUTFile=MPILeachRateLUTFile){

#Get the MPI Leach rate data
MPILeachRateRaster <- stack(LeachRateRasterFile)
#Get the MPI predictor data
MPIPredictorRaster <- stack(PredictorRasterFile)

  #Load MPI lookup table of total nitrogen leaching rates based on climate, land use, PAW, irrigation, irrigable land
  MPILeachRateLookUpTable<- read.csv(MPILeachRateLUTFile)
  #Check for duplicates!! This has ocurred in the past and caused an error later on which took hours to figure out!
  MPILeachRateLookUpTable <- MPILeachRateLookUpTable[!duplicated(MPILeachRateLookUpTable[,1:5]),]
  #concatenate all the predictor class values into a long string to provide a single column to look up.
  MPILeachRateLookUpTable$CombinedCriteria <- do.call(paste,MPILeachRateLookUpTable[1:5])
  
  if(!exists("MPILeachRateLookUpTable")){ MPILeachRateLookUpTable <- read.csv(MPILeachRateLUTFile)}
  
MPILeachRateLookUpTable$CombinedCriteria <- do.call(paste,MPILeachRateLookUpTable[1:5])

#Convert the raster stack to points
raspt <- rasterToPoints(MPIPredictorRaster)

#Discard the location columns and turn into a data frame
PredictorCombinations <- as.data.frame(raspt[,c(3:7)])

#Give the columns their correct names
names(PredictorCombinations) <- names(MPILeachRateLookUpTable)[1:5]

#Create a single column that combines all the predictor values into a string
PredictorCombinations$CombinedPredictors <- do.call(paste,PredictorCombinations[1:5])

#Count the number in each unique combination
CountInEachUniqueCombination <- table(PredictorCombinations$CombinedPredictors)

#Extract the unique combinations from the dimnames attribute of the counts
UniqueCombinations <- unlist(attr(CountInEachUniqueCombination,"dimnames"), use.names=FALSE)

#Get the N_loss rows that match the unique combinations
InfoOfInterest <- MPILeachRateLookUpTable[match(UniqueCombinations,MPILeachRateLookUpTable$CombinedCriteria),]
InfoOfInterest$Count <- as.vector(CountInEachUniqueCombination)
InfoOfInterest$areaHectares <- InfoOfInterest$Count * 6.25

#Ditch the "CombinedClass" and "ModificationComment" columns
InfoOfInterest <- subset(InfoOfInterest, select=-c(CombinedClass,ModificationComment))

#Save it
write.table(InfoOfInterest, file.path(DataDirectory,"MPIClassAreas.csv"),sep=",",row.names = FALSE,quote=FALSE)
}

 # Function to check that the output I generate from the scenarios is reasonable with respect to previously prepared data (perhaps the original data which was used to calibrate the CASM model).
 # The first check is to ensure that the river network, point sources and water quality stations are the same
 # The second check is to ensure the diffuse source names, locations and areas are the same
 # The third check is to compare the leach rates for the different sub management zone- land use - LUC areas and identify those that have changed.   
 # The fourth Check is to find the percent that they have changed by.
# The fifth check is to make sure the leach rates are between 0 and 75
#The function generates a list of three objects. The first two are single logic items, one for the intial check, one for the second check. The third object in the list is a table of the changed diffuse nodes and the percent amount they have change by
CASMScenarioInputChecker <- function(CASMCalibratedFile="D:\\Projects\\LWP\\Horizons\\Data\\CASM-Inputs_CoxCalLeach.xlsx", CASMFileToBeChecked="D:\\Projects\\LWP\\Horizons\\Data\\CASM-Inputs_FullTable14.2.xlsx") {
  if (!require("openxlsx")) install.packages("openxlsx"); library(openxlsx) #needed for reading excel spreadsheet

  #Read in the sheets of the first file
  Original <- lapply(c(1,2,3,4), function(sheet) {
    read.xlsx(CASMCalibratedFile, sheet = sheet)
  })
  
  #Read in the sheets of the second file
    DataToBeChecked <- lapply(c(1,2,3,4), function(sheet) {
    read.xlsx(CASMFileToBeChecked, sheet = sheet)
  })
  
  #Check that most of the sheets are identical
  Sheet1To3Same <- identical(Original[[1:3]],DataToBeChecked[[1:3]])  
  
  #Check that most of the Diffuse sheet is identical
  DiffuseNamesStreamsAndAreaSame <-
    identical(Original[[4]][,c(1,2,3,4)],DataToBeChecked[[4]][,c(1,2,3,4)])
  
  #Identify the locations that are not identical
  IndicesOfChange <- which((Original[[4]][,'Export.Coeff.(kg/ha/yr)'] - DataToBeChecked[[4]][,'Export.Coeff.(kg/ha/yr)']) != 0)
  
  #Determine the percent change for the different places
  PctChange <- (DataToBeChecked[[4]][,'Export.Coeff.(kg/ha/yr)'] - Original[[4]][,'Export.Coeff.(kg/ha/yr)']) / Original[[4]][,'Export.Coeff.(kg/ha/yr)'] * 100
  
  ChangeTable <- data.frame(Node.Name=Original[[4]]$Node.Name, PctChange=PctChange, stringsAsFactors = FALSE)
  
  #Check the rates are positive but less than 75
  WithinLimits <- all(DataToBeChecked[[4]][,'Export.Coeff.(kg/ha/yr)'] >= 0 & DataToBeChecked[[4]][,'Export.Coeff.(kg/ha/yr)'] < 75)
  
  ChangeResults = list(Sheet1To3Same = Sheet1To3Same,DiffuseNodeDetailsTheSame = DiffuseNamesStreamsAndAreaSame,ChangeTable[IndicesOfChange,],DiffuseWithinLimits = WithinLimits)
  return(ChangeResults)
}

#Function to find the areas that are sheep and beef generate a table of their attributes. On request of Dave Horne at Massey
SheepAndBeefDetails <- function(GISDataDirectory=GISDataDirectory,
                                              PredictorRasterFile=PredictorRasterFile,
                                              DataDirectory=DataDirectory,
                                              MPILeachRateLUTFile=MPILeachRateLUTFile){

#Load required libraries if required
if (!require(sp)) install.packages("sp"); library(sp)   
  
#Get the MPI predictor data
MPIPredictorRaster <- stack(PredictorRasterFile)

  #Load MPI lookup table of total nitrogen leaching rates based on climate, land use, PAW, irrigation, irrigable land
  MPILeachRateLookUpTable<- read.csv(MPILeachRateLUTFile)
  #Check for duplicates!! This has ocurred in the past and caused an error later on which took hours to figure out!
  MPILeachRateLookUpTable <- MPILeachRateLookUpTable[!duplicated(MPILeachRateLookUpTable[,1:5]),]
  #concatenate all the predictor class values into a long string to provide a single column to look up.
  MPILeachRateLookUpTable$CombinedCriteria <- do.call(paste,MPILeachRateLookUpTable[1:5])
  
MPILeachRateLookUpTable$CombinedCriteria <- do.call(paste,MPILeachRateLookUpTable[1:5])

#Convert the raster stack to points
raspt <- rasterToPoints(MPIPredictorRaster)

#Turn this into a spatial data file

#turn into a data frame
PredictorCombinations <- as.data.frame(raspt)

#Turn this into a spatial data frame
PredictorCombinations <- SpatialPointsDataFrame(coords = PredictorCombinations[,1:2], data = PredictorCombinations[,3:7],
                               proj4string = CRS("+init:epsg:2193"))

#Give the columns their correct names
names(PredictorCombinations@data)[1:5] <- names(MPILeachRateLookUpTable)[1:5]

#Create a single column that combines all the predictor values into a string
PredictorCombinations@data$CombinedPredictors <- do.call(paste,PredictorCombinations[1:5])

#Count the number in each unique combination
CountInEachUniqueCombination <- table(PredictorCombinations@data$CombinedPredictors)

#Extract the unique combinations from the dimnames attribute of the counts
UniqueCombinations <- unlist(attr(CountInEachUniqueCombination,"dimnames"), use.names=FALSE)

#Get the N_loss rows that match the unique combinations
InfoOfInterest <- MPILeachRateLookUpTable[match(UniqueCombinations,MPILeachRateLookUpTable$CombinedCriteria),]
InfoOfInterest$Count <- as.vector(CountInEachUniqueCombination)
InfoOfInterest$areaHectares <- InfoOfInterest$Count * 6.25



#Ditch the "CombinedClass" and "ModificationComment" columns
InfoOfInterest <- subset(InfoOfInterest, select=-c(CombinedClass,ModificationComment))

#Save it
write.table(InfoOfInterest, file.path(DataDirectory,"MPIClassAreas.csv"),sep=",",row.names = FALSE,quote=FALSE)
}

#Function to adhust the leach rate raster in accordance with a potato-growing scenario.
#a) Assume all LUC 1 to 3 land had potato growing every fifth year. This is Version = "All"
#b) Assume all Dairy and Sheep and Beef LUC 1 to 3 land had potato growing. This is Version = "Dairy and SHeep and Beef"
#This is achieved by finding the locations that meet the criteria, then adjusting their leach rates.
PotatoLeachRateCreator <- function(CurrentLeachRateRaster = LeachRate, PotatoLeachRate = 46, ZLLRaster = ZoneLanduseLUCRaster, ZoneLanduseLUC_LUTFile = ZoneLanduseLUCCode_To_ClassLUTFile, Version = "All") {

  #Load the ZoneLanduseLUC raster look up table that relates the raster's codes to a combined-class name
  ZoneLanduseLUC_LUT <- read.table(ZoneLanduseLUC_LUTFile,sep=",",header=TRUE)
  
  #Parse the Combined name into its constituent parts
  ZoneLanduseLUC_LUT$Zone <- as.factor(sub('-.*','',ZoneLanduseLUC_LUT$CombinedClassName))
  ZoneLanduseLUC_LUT$ZoneNo <- as.numeric(ZoneLanduseLUC_LUT$Zone)
  ZoneLanduseLUC_LUT$Landuse <- as.factor(sub('.*-(.*)-.*','\\1',ZoneLanduseLUC_LUT$CombinedClassName))
  ZoneLanduseLUC_LUT$LanduseNo <- as.numeric(ZoneLanduseLUC_LUT$Landuse)                                         
  ZoneLanduseLUC_LUT$LUC <- as.numeric(sub('.*-','',ZoneLanduseLUC_LUT$CombinedClassName))

  #reclasify ZoneLanduSeLUCRaster based on levels to the constituent parts
  LandUseRaster <- reclassify(ZLLRaster,as.matrix(ZoneLanduseLUC_LUT[,c('ID','LanduseNo')]))
  #SMZRaster <- reclassify(ZLLRaster,as.matrix(ZoneLanduseLUC_LUT[,c('ID','ZoneNo')]))
  LUCRaster <- reclassify(ZLLRaster,as.matrix(ZoneLanduseLUC_LUT[,c('ID','LUC')]))
  
  #Find the LUC 1 to 3 areas
  AreasOfInterest <- LUCRaster <= 3
  
  #Limit them to specific land uses, if required
  DairyLandUseClass <- which(levels(ZoneLanduseLUC_LUT$Landuse) == "Dairy")
  SAndBLandUseClass <- which(levels(ZoneLanduseLUC_LUT$Landuse) == "Sheep and/or Beef")
  if (Version == "Dairy and Sheep and Beef") {AreasOfInterest <- AreasOfInterest & 
    (LandUseRaster == DairyLandUseClass | LandUseRaster == SAndBLandUseClass)}
  
  #Recalculate the leach rate using the potato leach rate, for the areas of interest. The assumption is the potatoes are on a 1 in 5 cycle, so the leachrate is adjusted accordingly.
  NewLeachrates <-  ((CurrentLeachRateRaster * 0.8 + PotatoLeachRate * 0.2) * AreasOfInterest) +  (CurrentLeachRateRaster * !AreasOfInterest)

  return(NewLeachrates)
}

```



Set directories and data file names.

Four main type of files exist:
R data files
Spatial shape files (either points, lines or polygons) in ESRI format
Raster files in geotif format
Text files in comma separated variable format

Note that some of these are filenames of files that are created by the script, or, confusingly, may be created if deemed necesary.
```{r}
#Set the project directory
ProjectDirectory <- "D:\\Projects\\LWP\\Horizons"       #Tim Kerr's Windows Home laptop

#Set the data directory
DataDirectory     <- file.path(ProjectDirectory, "Data")

#Set the GIS directory
GISDataDirectory  <- file.path(DataDirectory,"GIS")

#Get some auxiliary R data file names
REC_MeanFlowFile  <- file.path(DataDirectory,"REC2MeanFlow.RData")  #This is an RData file
NitrogenLoadsFile <- file.path(DataDirectory,"N_ExpCoeff_Est_HZ_Feb20.rdata")

#Set the various shape files. These are held within the GIS directory in subdirectories with the same name as the shapefile
REC_rivers <- file.path(GISDataDirectory,"RECV2-Riverlines-Horizons","RECV2-Riverlines-Horizons")
#REC data is available from the MfE data service. See https://data.mfe.govt.nz/layer/51826-river-environment-classification-manawatu-2010/ and https://data.mfe.govt.nz/layer/51847-river-environment-classification-watershed-manawatu-2010/

SubZonesFile                       <- file.path(GISDataDirectory,"Water_Management_Subzones_cleaned","Water_Management_Subzones_cleaned")
LanduseLUCShapeFile                <- file.path(GISDataDirectory,"Landuse_LUC Original","Landuse_LUC Original")
SubZoneLanduseLUCShapeFile         <- file.path(GISDataDirectory,"ZoneLanduseLUC","ZoneLanduseLUC")                   #May be created if necesary
IrrigatedShapeFile                 <- file.path(GISDataDirectory,"irrigated-land-area-2017-Manawatu","irrigated-land-area-2017-Manawatu")
IrrigableShapefile                 <- file.path(GISDataDirectory,"IrrigableLand","Irrigable-Horizons")
PAWShapeFile                       <- file.path(GISDataDirectory,"FSL_Horizons","FSL_Horizons")
ClimateShapeFile                   <- file.path(GISDataDirectory,"ClimateRegions-Horizon","ClimateRegions-Horizon")
CASMModelDomainFile                <- file.path(GISDataDirectory,"CASMModelDomains","CASMModelDomains")
TargetSubZonesFile                 <- file.path(GISDataDirectory,"Target_subzones","Target_subzones")
MeasurementSitesFile               <- file.path(GISDataDirectory,"MeasurementSites","MeasurementSites")               #An output filename
PointSourceSitesFile               <- file.path(GISDataDirectory,"PointSourceSites","PointSourceSites")                #An output filename
WaterManagementZonesForMappingFile <- file.path(GISDataDirectory,"WaterManagementZonesForMapping","WaterManagementZonesForMapping")   #An output filename
CASMStreamNetworkFile              <- file.path(GISDataDirectory,"CASM-StreamNetwork","CASM-StreamNetwork")

#Set the various raster file names
PredictorRasterFile                <- file.path(GISDataDirectory,"PredictorRaster 250x250.tif")
#Baseline MPI leach rates
LeachRateRasterFile                <- file.path(GISDataDirectory,"MPILeachRateRaster 250x250.tif")
#Various scenario leach rates these have already been corrected for the Tim Cox leach rate calibration corrections
#LeachRateRasterFile                 <- file.path(GISDataDirectory,"ScenarioLeachRasters_IntensiveLanduseTable14-2.tif")
#LeachRateRasterFile                <- file.path(GISDataDirectory,"ScenarioLeachRastersFullTable14-2.tif")
#LeachRateRasterFile                <- file.path(GISDataDirectory,"ScenarioLeachRasters_IntensiveLanduseTable14_2_Year5.tif")
#LeachRateRasterFile                 <- file.path(GISDataDirectory,"ScenarioLeachRasters_IntensiveLanduseExistingTable14_2_Year20.tif")
#LeachRateRasterFile                 <- file.path(GISDataDirectory,"ScenarioLeachRasters_IntensiveLanduseExistingTable14_2_Year5.tif")

ZoneLanduseLUCRasterFile           <- "ZoneLanduseLUCRaster 250 x 250.tif"

#Set the various text file names
OutletReachNamesFile               <- file.path(DataDirectory,"OutletReachNames.csv") 
MPILeachRateLUTFile                <- file.path(DataDirectory,"MPI_Baseline_N_losses_v7.csv")
MPIClassNameLUTFile                <- file.path(DataDirectory,"MPI_LookupNamesNLosses_v5.csv")
LanduseToLanduseLUTFile            <- file.path(DataDirectory,"LanduseToLanduseLookUpTable.csv")
ZoneLanduseLUCCode_To_ClassLUTFile <- file.path(DataDirectory,"ZoneLanduseLUCCode_To_ClassLUT.csv")
LeachRateLimitsFileName            <- file.path(DataDirectory,"Table14_2.csv")
PointSourceSiteFile                <- file.path(DataDirectory,"PointSourcesSummaryPC2_YE2012_04.csv")

#Set some excel spreadsheet names
HorizonsConsentedLeachRatesFile    <- file.path(DataDirectory,
                                                "20200417_Master dairy farm data_Working Document – For Purposes of Scenario ModellingV4.xlsx")
CASMCalibrationSummaryFile         <- file.path(DataDirectory,"attenuation summaries.xlsx")  #Provided by Tim Cox, has the node-to-calibrated leachrates relationship for the nodes that he adjusted during calibration
CASMInputFile                      <- file.path(DataDirectory,"CASM-Inputs_20200513.xlsx")   #A recent "baseline" scenario version, either with or without the Tim Cox corrections
PointSourceExtraDataFile           <- file.path(DataDirectory,"pointSourcesTN_2012And2017.xlsx")
PointSourceYear                    <- "2012" #This is the default and has been used for everything except for one scenario, The other option is "2017"
```


load data
```{r}
#REC data has been sourced from the NIWA website. It has cut down versions of the attribute names, e.g. nzsegment is nzsgmnt. Keep an eye on this when comparing to other data sources that may have the full RECV2 attribute names.
RECReachNetwork <- readOGR(dsn = dirname(REC_rivers),layer = basename(REC_rivers))
#Explicitly set projection to NZTM
RECReachNetwork <- spTransform(RECReachNetwork,CRS("+init=epsg:2193") )

#Load the mean flow data for the REC V2. This is provided in an rdata file which contains the data frame REC2MeanFlow, with three columns, "nzsegment","QMean", and "us_catarea"
load(REC_MeanFlowFile)

#Load the CASM Model Domains spatial file
CASMModelDomains <- readOGR(dsn = dirname(CASMModelDomainFile),layer=basename(CASMModelDomainFile), stringsAsFactors = FALSE)
#Explicitly set projection to NZTM
CASMModelDomains <- spTransform(CASMModelDomains,CRS("+init=epsg:2193") )

#Load the Horizon total nitorigen load measurement sites, previously prepared by Caroline Fraser and called "HZLoads
load(NitrogenLoadsFile)
#Select just the TN sites
MeasurementSites <- HZLoads[which(HZLoads$npID=="TN"),]

#Calculate loads as t/y
MeasurementSites$load <- round(MeasurementSites$ExpCoef * MeasurementSites$CATCHAREA / 10000,2)
MeasurementSites$load_Uci <- round(MeasurementSites$ExpCoef_Uci_ * MeasurementSites$CATCHAREA / 10000,2)
MeasurementSites$load_Lci <- round(MeasurementSites$ExpCoef_Lci_ * MeasurementSites$CATCHAREA / 10000,2)

#Create a spatial points object set to NZTM
MeasurementSitesSpatial <-SpatialPointsDataFrame(coords = MeasurementSites[,c("NZTM.X","NZTM.Y")],
                                               data = MeasurementSites[,1:13],
                                               proj4string = CRS("+init=epsg:2193"))
#Find which model domain they are in
MeasurementSitesSpatial@data$CASMDomain <- (MeasurementSitesSpatial %over% CASMModelDomains)$CASMModel

#Save a copy for external use
writeOGR(MeasurementSitesSpatial,dsn = dirname(MeasurementSitesFile), layer = basename(MeasurementSitesFile),driver='ESRI Shapefile',overwrite_layer = TRUE)

#Load the leach rate maximums set out by Horizons in their plan, and ammended in their proposed plan change 2. This includes a 4 stepped change for year's 1, 5,10 and 20
LeachRateLimits <- read.table(LeachRateLimitsFileName, sep=",", header=TRUE)


#Load the point sources of nitrogen, previously prepared by Caroline Fraser. Unfortunately they don't have nzsgmnt data. note that an error was found for the NZTM.X position of the Fonterra at Pahiatua site. Originally it was 1859554.9 but it should be 1839554.9. This has been corrected on the LWP sharepoint version of the 20200201_HorizonsRiverCriteria.xlsx file, and the Tim Kerr version of the PointSourcesSummaryPC2_YE2012_04.csv file.
#PointSourceSites <- read.csv(PointSourceSiteFile)

#Alternative source of point source data is from a spreadsheet that Ton prepared. This has 2012 and 2017 data. 2012 data are the default data used for calibration and most scenarios unless specified otherwise.
PointSourceSites <- read.xlsx(PointSourceExtraDataFile, sheet = PointSourceYear)
#Select the columns of interest
PointSourceSites <- PointSourceSites[,c("Site.Name","nzsegment","Annual.load.kg/yr")]
#rename the columns to match those used in the original PointSourceSiteFile "PointSourcesSummaryPC2_YE2012_04.csv"
PointSourceSites <- rename(PointSourceSites,replace=c("Annual.load.kg/yr" = "TN.kgpy"))
#Select the rows of interest
PointSourceSites <- PointSourceSites[!(is.na(PointSourceSites$"TN.kgpy")|PointSourceSites$"TN.kgpy" ==0),]

#Add spatial data, derived from the down coordinate for the REC reach
PointSourceSites$NZTM.X <- RECReachNetwork$dwncrdX[match(PointSourceSites$nzsegment,RECReachNetwork$nzsgmnt)]
PointSourceSites$NZTM.Y <- RECReachNetwork$dwncrdY[match(PointSourceSites$nzsegment,RECReachNetwork$nzsgmnt)]

#Create a spatial points object set to NZTM
PointSourceSitesSpatial <-SpatialPointsDataFrame(coords = PointSourceSites[,c("NZTM.X","NZTM.Y")],
                                               data = PointSourceSites[,1:5],
                                               proj4string = CRS("+init=epsg:2193"))
#Find which catchment each point is in
PointSourceSitesSpatial@data$CASMDomain <- (PointSourceSitesSpatial %over% CASMModelDomains)$CASMModel

#If needed, save a copy for external use
#writeOGR(PointSourceSitesSpatial,dsn = dirname(PointSourceSitesFile),layer=basename(PointSourceSitesFile),driver='ESRI Shapefile',overwrite_layer = TRUE)

#Load the subzone polygons
SubZonePolygons <- readOGR(dsn=dirname(SubZonesFile),layer = basename(SubZonesFile))
SubZonePolygons <- spTransform(SubZonePolygons,CRS("+init=epsg:2193") )

# #For mapping purposes it is useful to know which CASM model each zone is within. This is (mostly) done by the following few lines. However, for zones bordering two models, it sometimes selectes the wrong model domain, so this file has been externally manually edited. Be sure that you want to overwrite it!
# #Add an attribute showing which model domain a zone is within
# SubZonePolygons@data$CASMDomain <- (SubZonePolygons %over% CASMModelDomains)$CASMModel
# 
# #Save a copy for external use
# writeOGR(SubZonePolygons,dsn = dirname(WaterManagementZonesForMappingFile),layer=basename(WaterManagementZonesForMappingFile),driver='ESRI Shapefile',overwrite_layer = FALSE)

#Load the network outlet reach names lookup table. This has been manually prepared, and may need editing if the network changes to include outlet reaches not yet included in this file
OutletReachNames <- read.csv(OutletReachNamesFile, stringsAsFactors = FALSE)

#Load the spatial data with the landuse, land use capability and submanagement zones alltogether
#LanduseLUCSpatial <- readOGR(dsn = dirname(LanduseLUCShapeFile),layer = basename(LanduseLUCShapeFile))
#LanduseLUCSpatial <- spTransform(LanduseLUCSpatial,CRS("+init=epsg:2193") )
SubZoneLanduseLUCSpatial <- readOGR(dsn= dirname(SubZoneLanduseLUCShapeFile), layer=basename(SubZoneLanduseLUCShapeFile))
SubZoneLanduseLUCSpatial <- spTransform(SubZoneLanduseLUCSpatial,CRS("+init=epsg:2193") )

#Create a Special case where all landuse is changed to native. This is for a scenario output
#SubZoneNativeLanduseLUCSpatial <- SubZoneLanduseLUCSpatial
#SubZoneNativeLanduseLUCSpatial$RegiScLand[SubZoneNativeLanduseLUCSpatial$RegiScLand != "Water Body"] <- #levels(SubZoneNativeLanduseLUCSpatial$RegiScLand)[7]
#writeOGR(SubZoneNativeLanduseLUCSpatial,dsn = file.path(GISDataDirectory,"SubZoneNativeLanduseLUC"),
#         layer = "SubZoneNativeLanduseLUC",
#         driver='ESRI Shapefile',
#         overwrite_layer = TRUE)
#SubZoneLanduseLUCSpatial <- SubZoneNativeLanduseLUCSpatial

#Create another special case where there is an 11 % expansion of dairy (from sheep and beef)


#Load the leachingrate data if it exists
LeachRate <- stack(LeachRateRasterFile)[[1]]

#**********************************************************
#If the leach rate raster file is missing or needs to be re-created then use the following:
#Takes 5 minutes at 250 m
#Note addition of timing to see how long it takes
# tic()
#LeachRate <- LeachRateRasterCreator(ClimateData = ClimateShapeFile,
#                                    LanduseData = file.path(GISDataDirectory,"SubZoneLanduseLUCShapeFile","SubZoneLanduseLUCShapeFile"),
#                                    IrrigatedLandData = IrrigatedShapeFile,
#                                    IrrigableLandData = IrrigableShapefile,
#                                   PAWData = PAWShapeFile,
#                                    MPILeachRateData = MPILeachRateLUTFile,
#                                    LanduseToLanduseLUT = LanduseToLanduseLUTFile)
#Use this one for an all native land use leach rate
# toc()
# #And save it for nexttime
# writeRaster(LeachRate,LeachRateRasterFile,overwrite=TRUE)
#******************************************************************
# Or for a leach rate raster associated with native landcover use:
#LeachRateRaster <- LeachRateRasterCreator(LanduseData = NativeLandUSe)

#Load the ZoneLanduseLUC raster if it exists
ZoneLanduseLUCRaster <- raster(ZoneLanduseLUCRasterFile, RAT=TRUE)
#Annoyingly the attribute table has a bonus line at the begining when loaded from the tif, so I've removed it to save pain later on
levels(ZoneLanduseLUCRaster)[[1]] <- levels(ZoneLanduseLUCRaster)[[1]][-1,]
#**********************************************************
##If the ZoneLanduseLUCRaster raster file is missing or needs to be re-created then use the following:
##Takes 5 minutes at 250 m
##Note addition of timing to see how long it takes
#tic()
#ZoneLanduseLUCRaster <- ZoneLanduseLUCRasterCreator(ZoneLanduseLUCPolygons = SubZoneLanduseLUCSpatial,LeachRates = LeachRate)
#toc()
#Save a copy for later
#writeRaster(ZoneLanduseLUCRaster,ZoneLanduseLUCRasterFile,overwrite=TRUE)
#browser()
#And save a levels-to-class name look up table
#write.table(levels(ZoneLanduseLUCRaster),ZoneLanduseLUCCode_To_ClassLUTFileName,sep=",",quote=FALSE, row.names=FALSE)
#******************************************************************


#If using the MPI baseline, override the calibrated values with the Tim Cox values
#This is a bit of a hack to get around the calibration of an input variable!
if (basename(LeachRateRasterFile) == "MPILeachRateRaster 250x250.tif") {
  #Override the MPI-derived leachrate raster with one that has been tweaked during calibration.
  #Background
  #When Tim Cox was calibrating his model he found that the attenuation rates that he needed to get catchment nutrient balance were smaller than what you would expect. So he decreased the leach rates to make the attenuation rates more believeable.
  #This became his calibrated baseline leach rates.
  #When implementing alternative scenarios, it is necesary to ensure that Tim Cox's leach rates are used.
  #Tim Cox's leach rates are by sub management class-landuse-LUC.
  #So they can be used as a lookup table from the ZoneLanduseLUCRaster to create a "calibrated baseline" leach rate raster
  
  #Start by loading Tim Cox's leach rates (he calls them Export Coefficients)
  CoxCalibratedLeachRateLUT <- read.xlsx(CASMCalibrationSummaryFile, sheet = "calibrated parameters",startRow=2,cols=c(2,6))
  #Unfortunately this is missing a few nodes, so we need to load in the most recent CASM input" file and make sure all the nodes are represented.
  #This is all a bit circuitous and couldn't be done if we were starting afresh. 
  RecentLeachRateLUT <- read.xlsx(CASMInputFile,sheet = "Diffuse Inputs",cols=c(1,5))
  #Update the recent look up table with values from the calibrated" look up table
  RecentLeachRateLUT$`Export.Coeff.(kg/ha/yr)`[match(CoxCalibratedLeachRateLUT$Node.Name,RecentLeachRateLUT$Node.Name)] <- CoxCalibratedLeachRateLUT$`Export.Coeff.(kg/ha/yr)`
  #And rename this to be the CalibratedLeachRaetLUT
  CalibratedLeachRateLUT <- RecentLeachRateLUT
  
  #I need to extend the lookup table to include the raster values. I can do that by using the raster attribute table
  
  CalibratedLeachRateLUT$RasterValue <- ZoneLanduseLUCRaster@data@attributes[[1]]$ID[match(CalibratedLeachRateLUT$Node.Name,ZoneLanduseLUCRaster@data@attributes[[1]]$category)]
  
  #I can now use this to reclass the raster
  LeachRate <- subs(ZoneLanduseLUCRaster, CalibratedLeachRateLUT,by="RasterValue",which="Export.Coeff.(kg/ha/yr)")
}

```



Some scenario data preparation.
Chunk  to read in the Horizons property attributes table and to pare it down to useful stuff.
****This processing relies on the sub management zones provided in a spreadsheet from Horizons. Unfortunately some of those sub management zones have changed (split). The location information for the farms is available from "Master farm data for PC 2 evidence streams.xlsx within 
https://harrcons.sharepoint.com/sites/HorizonsPlanChange_2 under Documents/Analysis/LeachingRates
This location information needs to be used to update some of theses sites' attributes. 
```{r}
#Load a couple of special libraries
if (!require("openxlsx")) install.packages("openxlsx"); library(openxlsx) #needed for reading excel spreadsheet
if (!require("tidyr")) install.packages("tidyr"); library(tidyr)  #has the "pivot_long" function

HorizonsPropertyData <- read.xlsx(HorizonsConsentedLeachRatesFile,sheet = "Dairy farms - target catchments", startRow = 3)

#Add some descriptive column names
LUCAreaColumnNames <- paste0("AreaInLUC",seq(1:8))
names(HorizonsPropertyData)[23:30] <- LUCAreaColumnNames

#Create a new column that meets the Federated Farmers criteria of the lesser of consented, or 90 % of the overseer base values, or the 75th percentile of the Overseeer base vaues for the management zone
FedFarmValues <- apply(HorizonsPropertyData,1, function(x) {
  WaterManagementZone <- x["Water.Management.Zone"]
  WMZBaselines <- HorizonsPropertyData$`Base.N.Leaching.(kg.N/ha/yr)`[HorizonsPropertyData$Water.Management.Zone == WaterManagementZone]
  Percentile75th <- quantile(WMZBaselines,probs = 0.75,names=FALSE,na.rm=TRUE)
  NewLossRate <- if(x["Consented"] == "Y") as.numeric(x["Target.N.Leaching.(kg.N/ha/yr)"]) else min(Percentile75th,0.9 * as.numeric(x["Base"]),na.rm=TRUE)
})

#Pare it down to just the data that is needed
HorizonsPropertyData <- HorizonsPropertyData[,c(3,15,23:30,47:59)]

#Add on the FedFarmers Scenario
HorizonsPropertyData$Scenario.8.Fed.Farmers <- FedFarmValues

#there are some properties that are in multiple sub-management zones. Find them,divide them and share the LUC areas amongst the divided parts, adding the extra divided parts to the end of the master table
{
  MultiSubManagementZoneProperties <- which(grepl(",",HorizonsPropertyData$Water.Management.Subzone,fixed=TRUE))
  
  #Work through each of these multi-zone properties and divide them into single sub management zones rows
  for(MultiSMZIndex in MultiSubManagementZoneProperties){
    #Get the multi-sub-zones
    MultiSubZones <- unlist(strsplit(HorizonsPropertyData$Water.Management.Subzone[MultiSMZIndex], ","))
    #Clear them of whitespace
    MultiSubZones <- gsub(" ","",MultiSubZones)
    NoOfSMZs <- length(MultiSubZones)
    #Give the existing row the first name
    HorizonsPropertyData$Water.Management.Subzone[MultiSMZIndex] <- MultiSubZones[1]
    
    #Divide the LUC areas by the number of SMZ's
    HorizonsPropertyData[MultiSMZIndex,LUCAreaColumnNames] <- 
      HorizonsPropertyData[MultiSMZIndex,LUCAreaColumnNames] / NoOfSMZs
    #Duplicate this for each of the other SMZ's but change the SMZ, and tack it on the end of the data
    for (SMZIndex in seq(from=2,to=NoOfSMZs)){
      NewRow <- HorizonsPropertyData[MultiSMZIndex,]
      NewRow$Water.Management.Subzone <- MultiSubZones[SMZIndex]
      HorizonsPropertyData <- rbind(HorizonsPropertyData,NewRow)
    }
  }
}

#Go from wide to long, converting the LUC_area columns to just area, with extra rows for each LUC
data_long <- pivot_longer(HorizonsPropertyData,cols=starts_with("AreaInLUC"),names_to="LUC",names_prefix = "AreaInLUC",values_to="hectares")
#Remove all the rows with no area
data_long <- data_long[data_long$hectares > 0,]

#Create a combined sub management zone, landuse, LUC name
data_long$CombinedName <- paste0(data_long$Water.Management.Subzone,"-Dairy-",data_long$LUC)

#browser()
#Calculate the diffuse load for each "Combined name" class for the scenario of interest. (This could be ammended to automagically work through each scenario)
#ScenarioColumnOfInterest <- "Scenario.2(a).-.9kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.2(b).-.12kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.2(c).-.15kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.2(d).-.18kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.3(e).-.9kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.3(f).-.12kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.3(g).-.15kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.3(h).-.18kg/ha.reduction"
#ScenarioColumnOfInterest <- "Scenario.1(a).New.Table.-.Y5"
#ScenarioColumnOfInterest <- "Scenarios.1(b).and.2(e).New.Table.-.Y20"
#ScenarioColumnOfInterest <- "Baseline.Existing.Table.-.Y20"
#ScenarioColumnOfInterest <- "Baseline.Existing.Table.-.Y5"
#ScenarioColumnOfInterest <- "Base.N.Leaching.(kg.N/ha/yr)"
ScenarioColumnOfInterest <- "Scenario.8.Fed.Farmers"
ScenarioColumnOfInterest <- "Scenario.10.-.LesserOfConsentedorPC2Table14.2"

#data_longV2 <- data_long
data_long$Load <- data_long$hectares*data_long[,ScenarioColumnOfInterest]
#data_longV2$Load <- data_long$hectares*data_long[[ScenarioColumnOfInterest]]

#Sum the LUC areas and loads for each sub management zone, but don't sum the areas for the properties without leach data (e.g. non-consented properties when considering consented leach rates)
CombinedClasses  <- aggregate(data_long[,c("hectares","Load")]*!is.na(data_long$Load), by = list(data_long$CombinedName), FUN = sum, na.rm=TRUE)
#rename the aggregated column names
names(CombinedClasses) <- c("CASMNodeNames","Area_hectares","Load")
CombinedClasses$LeachRates <- CombinedClasses$Load / CombinedClasses$Area_hectares
```


I need to analyse each management sub zone to determine how much area (in hectares) each landuse-LUC combination takes up.
Unfortunately there is a descrepancy between provided data for the sub management zones. Upon query to Horizons (See email exchange between Ton Snelder and Stephen Collins- Horizons on 17th Feb 2020), the zones within the "Water_Management_Subzones" ESRI shapefile trump those within the "Landuse_LUC Original". So the land use and LUC is extracted from one file, then intersected with the other.This proved to be problematic because both files had topology errors, primarily small circles in the boundaries, so the files had to be "cleaned". This proved to be most effectively done in QGIS. They were then intersected in QGIS, and then aggregated by RegiScLand, LUC_CLASS and Zone_Code in R (see below) before being saved as a file so that the painfull process doesn't have to be repeated.
```{r}

#Intersect the Landuse and LUC with the sub-management zones
#SubZoneLanduseLUCSpatial <- intersect(LanduseLUCSpatial,SubZonePolygons)

#Aggregate on sub_zone, Landuse and LUC and save for later
#SubZoneLanduseLUCSpatial <- aggregate(SubZoneLanduseLUCSpatial,by=c("RegiScLand","LUC_CLASS","Zone_Code"))
#writeOGR(SubZoneLanduseLUCSpatial,layer=SubZoneLanduseLUCShapeFile,driver='ESRI Shapefile')

#Find the area of each Landuse-LUC combination for each sub-zone
SubZoneLanduseLUCSpatial$hectares <- round(raster::area(SubZoneLanduseLUCSpatial) / 10000,1)

#Create a new combined name
SubZoneLanduseLUCSpatial$CombinedClassName <- with(SubZoneLanduseLUCSpatial@data, paste0(Zone_Code,"-",RegiScLand,"-",LUC_CLASS))

#Now break this into each individual polygon. I need to do this before getting the leach rates so that I don't average the leach rates without accounting for area. This is an alternative to using weights when doing the extract.

# #Start with a test for a single area
# Whau_3b <- SubZoneLanduseLUCSpatial[SubZoneLanduseLUCSpatial$Zone_Code == "Whau_3b",]
# Whau_3b_Sf <-as(Whau_3b, "sf")
# Whau_3b_Sf_single = st_cast(Whau_3b_Sf,"POLYGON")
# 
# Whau_3b_leach <- raster::extract(LeachRate, Whau_3b, fun = mean, na.rm=TRUE, sp=TRUE)
# Whau_3b_leach_weighted <- raster::extract(LeachRate, Whau_3b, fun = mean, na.rm=TRUE, sp=TRUE,weights = TRUE)
# 
# Whau_3b_Sf_single_leach <- raster::extract(LeachRate, Whau_3b_Sf_single, fun = mean, na.rm=TRUE, sp=TRUE)
# Whau_3b_Sf_single_leach$hectares <- raster::area(Whau_3b_Sf_single_leach) / 10000
# 
# Whau_3b_Sf_single_leach_weighted <- raster::extract(LeachRate, Whau_3b_Sf_single, fun = mean, na.rm=TRUE, sp=TRUE, weights = TRUE)
# Whau_3b_Sf_single_leach_weighted$hectares <- raster::area(Whau_3b_Sf_single_leach_weighted) / 10000

#I found that the sum of the loads (leach rate x area) after weighted extraction is very close to the value determined from the raster. When I repeat using singlepart polygons, the number is simmilar.
#The conclusion is the weighting is required, and that it works without having to convert to single parts.

##Get the average leach rate for each polygon THIS TAKES ABOUT 10 MINUTES for 250 x 250 m
#leachvalues <- raster::extract(LeachRate, SubZoneLanduseLUCSpatial, fun = mean, na.rm=TRUE, sp=TRUE, weights=TRUE)
```


Get the "nzsgmnt" attributes of the lowest reach in each of the management subzones
```{r}
SubZoneOfEachReach <- RECReachNetwork %over% SubZonePolygons #Note this takes about a minute to do
RECReachNetwork$SubZoneCode <- SubZoneOfEachReach$Zone_Code

#Work through each management zone to find the reach with the greatest distance to the headwater (the hdw_dst attribute). . I tried doing this based on the least largest area (the CUM_ARE attribute) but there were some zero values, I also tried distance to the sea (the LENGTHD attribute, but near the coast I was getting small reaches that were not the main river channel, but were closer to the sea)
ManagementSubZoneOutletReaches <- lapply(seq_along(SubZonePolygons$Zone_Code), function(SingleSubZoneIndex){
  #browser()
  CurrentSubZone <- SubZonePolygons$Zone_Code[SingleSubZoneIndex]
  CurrentSubZoneReaches <- RECReachNetwork[RECReachNetwork$SubZoneCode == CurrentSubZone,]
  OutletReach <- CurrentSubZoneReaches$nzsgmnt[which.max(CurrentSubZoneReaches$hdw_dst)]
  return(data.frame(SubZone = CurrentSubZone,nzsgmnt = OutletReach))
})
#Convert the list into a dataframe
ManagementSubZoneOutletReachesDF <- do.call(rbind,ManagementSubZoneOutletReaches)

#Save for later if required
#write.table(ManagementSubZoneOutletReachesDF, file.path(DataDirectory,"WaterManagementSubZone_LowestREC2nzsegment.csv"),sep=",",row.names = #FALSE,quote=FALSE,col.names=c("WMSZ","nzsegment"))
```


From the load sites, point source sites, and water management subzones outlets, create the required network
```{r}
AllPoints <- c(MeasurementSites$nzsegment, PointSourceSites$nzsegment, ManagementSubZoneOutletReachesDF$nzsgmnt)
LoadNetwork <- lapply(AllPoints, function(x) {DownstreamReachFinder(RECNetwork = RECReachNetwork, SourceReach = x)} )
CompleteNetwork <- unlist(LoadNetwork)
CompleteNetwork <- CompleteNetwork[!duplicated(CompleteNetwork)]

CompleteSpatialNetwork <- RECReachNetwork[RECReachNetwork$nzsgmnt %in% CompleteNetwork,]

CompleteSpatialNetwork@data$CASMDomain <- (CompleteSpatialNetwork %over% CASMModelDomains)$CASMModel
```

Then create a tributary table ready for CASM
```{r}

NetworkLabelList <- NetworkLabeler(CompleteSpatialNetwork)


#Add the tributary labels to the network
SegmentToLabelLookUpTable <- do.call(rbind,NetworkLabelList)
CompleteSpatialNetwork@data$Label <- SegmentToLabelLookUpTable$Label[match(CompleteSpatialNetwork@data$nzsgmnt,SegmentToLabelLookUpTable$nzsgmnt)]
#Add the prefixed labels to the network
SegmentToPrefixedLabelLookUpTable <- ReachLabeler(NetworkLabelList, OutletReachNames)
CompleteSpatialNetwork@data$PrefixedLabel <- SegmentToPrefixedLabelLookUpTable$Prefixedlabel[match(CompleteSpatialNetwork@data$nzsgmnt,SegmentToPrefixedLabelLookUpTable$nzsgmnt)]

#Add the 
#Save the network as a spatial file
writeOGR(CompleteSpatialNetwork, dsn = dirname(CASMStreamNetworkFile), layer= basename(CASMStreamNetworkFile), driver="ESRI Shapefile",overwrite_layer=TRUE)

TributaryConnectionTable <- TributaryConnectionCreator(RECNetwork = CompleteSpatialNetwork, TributaryLabelList = NetworkLabelList)
```

Then create a point source table ready for CASM,
a measurement site table ready for CASM,
and a diffuse inputs table ready for CASM
```{r}
#Create a dataframe of just the nzsegment number and the site name
PointSourceNodes <- PointSourceSites[,c("Site.Name","nzsegment")]

#Standardise the column names so that it matches the expected format in the CASMNodeTablePreparer() function
names(PointSourceNodes) <- c("NodeName","nzsgmnt")

#Prepare the CASM table with the bonus nzsgmnt column
PointSourceTable <- CASMNodeTablePreparer(CASMRECNetwork = CompleteSpatialNetwork, NetworkLabelList = NetworkLabelList, TributaryConnectionTable = TributaryConnectionTable,CASMNodes = PointSourceNodes )

#rename the columns to match the CASM requirements
names(PointSourceTable) <- c("nzsgmnt","Point Source Name","Receiving Stream","Location (km)")
 
#Need to add the point source load to the table
PointSourceTable$"Annual Load (kg/yr)" <- round(PointSourceSites$TN.kgpy[match(PointSourceTable$nzsgmnt,PointSourceSites$nzsegment)],0)

#For the native use, not point sources
#PointSourceTable$"Annual Load (kg/yr)" <- 0
```



#Repeat for the measurement sites
```{r}
MeasurementSiteNodes <- MeasurementSites[,c("sID","nzsegment")]
names(MeasurementSiteNodes) <- c("NodeName","nzsgmnt")
MeasurementSiteTable <- CASMNodeTablePreparer(CASMRECNetwork = CompleteSpatialNetwork, NetworkLabelList = NetworkLabelList, TributaryConnectionTable = TributaryConnectionTable,CASMNodes= MeasurementSiteNodes)

#rename the columns to match the CASM conventions
names(MeasurementSiteTable) <- c("nzsgmnt","Site Name or No","Target Stream","Downstream Location (km)")

#Add the mean annual flow to the table
MeasurementSiteTable$"Mean flow (m3/s)" <- round(REC2MeanFlow$QMean[match(MeasurementSiteTable$nzsgmnt, REC2MeanFlow$nzsegment)],1)

#Add a variety of other fields from the Measurement Site data
MeasurementSiteTable$'Load (kg/yr)' <- round(MeasurementSites$load[match(MeasurementSiteTable$"Site Name or No", MeasurementSites$sID)],1)
MeasurementSiteTable$'Load Lci (kg/yr)' <- round(MeasurementSites$load_Lci[match(MeasurementSiteTable$"Site Name or No", MeasurementSites$sID)],1)
MeasurementSiteTable$'Load Uci (kg/yr)' <- round(MeasurementSites$load_Uci[match(MeasurementSiteTable$"Site Name or No", MeasurementSites$sID)],1)
```

#Repeat for diffuse inputs. This is a special case, because once the points have been found, they need to be joined with all the different landuse/LUC options
```{r}
#Start with all the diffuse source input nodes. These are the sub-management zone outlets
DiffuseInputsSiteNodes <- ManagementSubZoneOutletReachesDF
names(DiffuseInputsSiteNodes) <- c("NodeName","nzsgmnt")

#Build up the table of tributary names and locations associated with the diffuse source input sites
DiffuseInputsSiteTable <- CASMNodeTablePreparer(CASMRECNetwork = CompleteSpatialNetwork, NetworkLabelList = NetworkLabelList, TributaryConnectionTable = TributaryConnectionTable,CASMNodes= DiffuseInputsSiteNodes)

#Now create a bigger version, with a row for each of the different landuse/LUC combinations within each sub-management zone
DiffuseInputsSiteExtendedTable <- SubZoneLanduseLUCSpatial@data

#Add the locations that we have just previously determined
DiffuseInputsSiteExtendedTable[,c("TribLocn","TribName")] <- DiffuseInputsSiteTable[match(SubZoneLanduseLUCSpatial@data$Zone_Code, DiffuseInputsSiteTable$CASMNodeName),c("TribLocn","TribName")]

#Add the leach rates
DiffuseLeachRateData <- DiffuseLoadTableCreator(ZoneLanduseLUCRaster = ZoneLanduseLUCRaster,LeachRates = LeachRate)
DiffuseInputsSiteExtendedTable$"LeachRates"<- DiffuseLeachRateData$LeachRate[match(DiffuseInputsSiteExtendedTable$CombinedClassName,DiffuseLeachRateData$CombinedClassName)]

#Need to use raster-calculated area rather than polygon areas
DiffuseInputsSiteExtendedTable$`RasterBased Land Area (ha)` <- DiffuseLeachRateData$Hectares[match(DiffuseInputsSiteExtendedTable$CombinedClassName,DiffuseLeachRateData$CombinedClassName)]
#Remove all the surplus combined names. The raster version has much less.
DiffuseInputsSiteExtendedTable <- DiffuseInputsSiteExtendedTable[complete.cases(DiffuseInputsSiteExtendedTable),]

#I now need to adjust the locations of all the different landuse/LUC options for a single diffuse source point so that they are not all on exactly the same spot.
#Work along each sub management zone, get all the landuse/LUC classes, and increment the locations by 0.1 km
DiffuseInputsSiteExtendedTable$AdjustedTriblocn <- DiffuseInputsSiteExtendedTable$TribLocn
UniqueSubZones <- unique(DiffuseInputsSiteExtendedTable$Zone_Code)
for(SubZoneIndex in seq_along(UniqueSubZones)) {
  #get the subzone of interest
  SubZone <- UniqueSubZones[SubZoneIndex]
  
  #Get all the landuse/LUC classes in the subzone
  RowsOfInterest <- which(DiffuseInputsSiteExtendedTable$Zone_Code == SubZone)
  DiffuseInputsSiteExtendedTable$AdjustedTriblocn[RowsOfInterest] <- DiffuseInputsSiteExtendedTable$TribLocn[RowsOfInterest] + seq(0,by = 0.01, length.out = length(RowsOfInterest))
}

#rename the columns to match the CASM conventions
names(DiffuseInputsSiteExtendedTable) <- c("Landuse","LUC","Zone_Code","Polygon Land Area (ha)","Node Name","Original location (km)", "Receiving Stream","Export Coeff (kg/ha/yr)","Land Area (ha)","Discharge Location (km)")

```

For Horizon's Scenario 5, an expansion of Dairy by 11 % is required in each water management zone.
The following assumes that dairy areas of the same LUC class as the sheep and beef areas already exist. This appears to be the case.

```{r}
#Find the amount of dairy in each water management sub zone
DairyConversionData <- DiffuseInputsSiteExtendedTable
DairyAreas <- aggregate(DairyConversionData[,c("Land Area (ha)")], by = list(Landuse = DairyConversionData[,"Landuse"] == "Dairy",Zone_Code = DairyConversionData[,"Zone_Code"]),FUN = sum)

#Find 11 % of the dairy areas.
DairyAreas <- DairyAreas[which(DairyAreas$Landuse),]
DairyAreas$Dairy.11pct <- DairyAreas$x * 0.11

#Find the amount of area in each sheep and beef/LUC combination in each water management sub zone.

#Within each water management sub zone, and starting from LUC 1 working up, subtract area from the sheep and beef and add it to dairy, until either the sheep and beef/LUC area is 0, or the total reduction of sheep and beef area matches the 11 % of dairy
for (WaterManagementSubZone in DairyAreas$Zone_Code){
  AreaToConvert <- DairyAreas$Dairy.11pct[DairyAreas$Zone_Code == WaterManagementSubZone]
  #browser()
  CurrentZoneData <- DairyConversionData[DairyConversionData$Zone_Code == WaterManagementSubZone,]
  CurrentZoneSheepAndBeefAreas <- CurrentZoneData[CurrentZoneData$Landuse == "Sheep and/or Beef",]
  CurrentZoneDairyAreas        <- CurrentZoneData[CurrentZoneData$Landuse == "Dairy",]

  #Work through the LUC classes moving land areas from sheep and beef to Dairy until enough conversions have ocurred
  LUCClasses <- sort(CurrentZoneSheepAndBeefAreas$LUC)
  for (LUC in LUCClasses) {
    while (AreaToConvert > 0){
      #browser()
    SAndBNodeOfInterest <- CurrentZoneSheepAndBeefAreas[CurrentZoneSheepAndBeefAreas$LUC == LUC,]
    DairyNodeOfInterest <- CurrentZoneDairyAreas[CurrentZoneDairyAreas$LUC == LUC,]
    DairyAreaInThisLUC <- DairyNodeOfInterest$"Land Area (ha)"
    AreaAvailableForConversion <- SAndBNodeOfInterest$"Land Area (ha)"
    RemainingSandBArea <- AreaAvailableForConversion - AreaToConvert
    
    #Lots of different ways for the conversion to work out
    #If there is enough land area in the sheep and beef, and there exists a node for the dairy of the same LUC then simply move the areas
    if (RemainingSandBArea >= 0) {
      DairyConversionData$NewLandArea[DairyConversionData$`Node Name` == SAndBNodeOfInterest$'Node Name'] <- RemainingSandBArea
      DairyConversionData$NewLandArea[DairyConversionData$`Node Name` == DairyNodeOfInterest$'Node Name'] <- DairyAreaInThisLUC + AreaToConvert
      AreaToConvert <- 0
    }
    else {
      DairyConversionData$NewLandArea[DairyConversionData$`Node Name` == SAndBNodeOfInterest$'Node Name'] <- 0
      DairyConversionData$NewLandArea[DairyConversionData$`Node Name` == DairyNodeOfInterest$'Node Name'] <- DairyAreaInThisLUC + AreaAvailableForConversion
      AreaToConvert <- AreaToConvert - AreaAvailableForConversion
    }
    } #End of while loop
  } #End of LUC for loop
} #End of water management zone for loop

#Select just the nodes with changed areas
DairyConversionUpdateData <- DairyConversionData[!is.na(DairyConversionData$NewLandArea),]

#Update the DiffuseInputsSiteExtendedTable land area values with the new land areas
DiffuseInputsSiteExtendedTable$`Land Area (ha)` <- DairyConversionUpdateData$NewLandArea[match(DiffuseInputsSiteExtendedTable$`Node Name`,DairyConversionUpdateData$`Node Name`)]
```


Adjust the diffuse inputs for any scenario data. This is prepared earlier in a data frame called "combinedclasses".
Adjustment is done for each node class that exists in the "CombinedClasses" dataframe.
The leach load is found for that part of the node's area that is not accounted for in the "combinedclasses". This is added to the "Combinedclasses" load, and the total is divided by the node area to return it to a rate.
*****Only run this chunk if you want special scenario stuff output********

```{r}
CombinedClasses$TotalNodeArea <- DiffuseInputsSiteExtendedTable$'Land Area (ha)'[match(CombinedClasses$CASMNodeNames,DiffuseInputsSiteExtendedTable$'Node Name')]
CombinedClasses$NodeAveLeachRate <- DiffuseInputsSiteExtendedTable$"Export Coeff (kg/ha/yr)"[match(CombinedClasses$CASMNodeNames,DiffuseInputsSiteExtendedTable$'Node Name')]
CombinedClasses$AreaDiff <- CombinedClasses$TotalNodeArea - CombinedClasses$Area_hectares
#Make sure the difference between the node area and the property area is not negative.
#If it is, then assume that the land is from a "Sheep and/or Beef" land use (i.e. a new conversion or misclassified land)
#Work through each and fix up.

OverAreaPropertyIndices <- which(CombinedClasses$AreaDiff < 0)

for(OverAreaPropertyIndex in OverAreaPropertyIndices){
  #Duplicate the property details
  DuplicateProperty <- CombinedClasses[OverAreaPropertyIndex,]
  #Convert the NodeName to be a Sheep and/or Beef one
  DuplicateProperty$CASMNodeNames <- sub("Dairy","Sheep and/or Beef",DuplicateProperty$CASMNodeNames)
  #Update the TotalNodeArea and NodeAveLeachRate from the original DiffuseInput data
  DuplicateProperty$TotalNodeArea <- DiffuseInputsSiteExtendedTable$'Land Area (ha)'[match(DuplicateProperty$CASMNodeNames,DiffuseInputsSiteExtendedTable$'Node Name')]
  DuplicateProperty$NodeAveLeachRate <- DiffuseInputsSiteExtendedTable$"Export Coeff (kg/ha/yr)"[match(DuplicateProperty$CASMNodeNames,DiffuseInputsSiteExtendedTable$'Node Name')]
  #Update the Area_hectares attribute with the left overs from the dairy area comparison
  DuplicateProperty$Area_hectares <- DuplicateProperty$AreaDiff * -1
  #Double check that we haven't still run out of area
  DuplicateProperty$AreaDiff <- DuplicateProperty$TotalNodeArea - DuplicateProperty$Area_hectares
  
  #Any further over areas put down to misclassifications that can't be resolved, so adjust the property areas for them to match the node total area
  DuplicateProperty$AreaDiff[DuplicateProperty$AreaDiff < 0] <- 0
  DuplicateProperty$Area_hectares[DuplicateProperty$AreaDiff == 0] <- DuplicateProperty$TotalNodeArea
  #Tack the duplicate onto the end
  CombinedClasses <- rbind(CombinedClasses,DuplicateProperty)
}

#Now get the total load and convert back to a rate
CombinedClasses$NewRate <- with(CombinedClasses, (LeachRates * pmin(Area_hectares,TotalNodeArea) + NodeAveLeachRate * pmax(0,AreaDiff)) / TotalNodeArea)

#Check which property node types don't exist in the main table
ExtraNodeTypes <- unique(CombinedClasses$CASMNodeNames)[which(!unique(CombinedClasses$CASMNodeNames) %in% DiffuseInputsSiteExtendedTable$`Node Name`)]

#...and ditch them!
CombinedClasses <- CombinedClasses[!CombinedClasses$CASMNodeNames %in% ExtraNodeTypes,]



#Then use this to update the node's leach rate
DiffuseInputsSiteExtendedTable$PreScenarioAdjustLeachRate <- DiffuseInputsSiteExtendedTable$"Export Coeff (kg/ha/yr)"
DiffuseInputsSiteExtendedTable$"Export Coeff (kg/ha/yr)"[match(CombinedClasses$CASMNodeNames,DiffuseInputsSiteExtendedTable$'Node Name')] <- CombinedClasses$NewRate

```


plot
```{r}
#Reproject the other spatial data to the maps projection ready for plotting
SpatialData <- list(MeasurementSites=MeasurementSitesSpatial, PointSourceSites=PointSourceSitesSpatial,
                    SubZones=SubZonePolygons,RiverNetwork=CompleteSpatialNetwork)
reprojected.data.WGS84 <- lapply(SpatialData, spTransform,CRS("+init=epsg:4326"))


map <- leaflet::leaflet() %>% 
  leaflet::addProviderTiles(providers$OpenStreetMap) %>%
  setView(lng=175.5,lat=-40.0,zoom=8) %>% 
  addPolygons(data = reprojected.data.WGS84$SubZones, color = "black", weight = 3, fillColor = "transparent", label = ~htmlEscape(Zone_Code)) %>%
  addCircleMarkers(data = reprojected.data.WGS84$MeasurementSites, color = "red",label = ~htmlEscape(sID)) %>%
  addCircleMarkers(data = reprojected.data.WGS84$PointSourceSites, color = "black", label = ~htmlEscape(Site.Name)) %>%
  addPolylines(data = reprojected.data.WGS84$RiverNetwork, color= "blue", label = ~htmlEscape(PrefixedLabel))
map

#save the mapdata as an R file so that it can be used in an RShinyApp
saveRDS(reprojected.data.WGS84,file.path(ProjectDirectory,"ShinyApp/Data","SpatialData.RDS"))
```


The tributary connection table needs to be converted to an Excel Spreadsheet.


I need to create an excel table of CASM-Nodes, CASM-Reach-Names, CASM-Reach-Locations, CASM-Reach-Areas, CASM-Reach-Exp.Coeff
```{r}


Out <- createWorkbook()

addWorksheet(Out, "River Network")

writeData(Out, sheet = "River Network", x = TributaryConnectionTable[c("Tributary Name","Confluence Stream","Confluence Location (km)")])

addWorksheet(Out, "Point Source")

writeData(Out, sheet = "Point Source", x = PointSourceTable[,-1])

addWorksheet(Out, "Water Quality Stations")

writeData(Out, sheet = "Water Quality Stations", x = MeasurementSiteTable[,-1])

addWorksheet(Out, "Diffuse Inputs")


writeData(Out, sheet = "Diffuse Inputs", x = DiffuseInputsSiteExtendedTable[,c("Node Name","Receiving Stream","Discharge Location (km)","Land Area (ha)","Export Coeff (kg/ha/yr)")])

saveWorkbook(Out, file.path(DataDirectory,"CASM-Inputs_Horizons_Scenario10_LesserOfConsentedOrTable14.2_MPICoxCalibrated.xlsx"), overwrite = T)
```
 By way of a check, it will be helpful to compare the total sub-management zone loads from the gridded leachrate data with the total sub-management zone loads from the zone/landuse/LUC data.
 Start with the leach rate raster data, and get the average leachrate for each zone and multiply by the area.
 Note that  I need to use the subzones from the SubZone
 
 Then, as a check, get the load for each zone-landuse-LUC combination, and sum in each zone.
 It would also be good to add a check of total load in catchments upstream of a water quality network being more than the total load measured. If they were less, then the attenuation would have to be greater than 1!
```{r}
#Note that to ensure I am comparing apples with apples, I am using the SubZoneLandUseLUCSpatial data to find the SubZone areas (rather than using the pre-prepared cleaned sub zone spatial data which has been "cleaned" so is slightly different in area!!)
test <- unionSpatialPolygons(SubZoneLanduseLUCSpatial,IDs = SubZoneLanduseLUCSpatial@data$Zone_Code )
testid <- sapply(slot(test, "polygons"), function(x) slot(x, "ID"))
test2 <- SpatialPolygonsDataFrame(test,data.frame(Zone_Code = testid,row.names = testid))

Zoneleachrates <- raster::extract(LeachRate, test2, fun = mean, na.rm=TRUE, sp=TRUE, weights = TRUE)
Zoneleachrates$area <- raster::area(Zoneleachrates)
Zoneleachrates$loads <- with(Zoneleachrates@data,layer * area / 10000)

DiffuseInputsSiteExtendedTable$load <- DiffuseInputsSiteExtendedTable$`Export Coeff (kg/ha/yr)` * DiffuseInputsSiteExtendedTable$`Land Area (ha)`
library(plyr)
bob <- ddply(DiffuseInputsSiteExtendedTable, "Zone_Code", function(x) sum(x$load, na.rm=TRUE))

ddply(DiffuseInputsSiteExtendedTable, "Zone_Code", function(x) sum(x$'Land Area (ha)', na.rm=TRUE))

#Ideally the loads from the raster and the DiffuseinputsSiteExtendedTable would be the same, but they are not!!
#But they are close for most water management sub-zones
charlie <- cbind(Zoneleachrates@data,bob)
charlie$load_diff <- charlie$loads - charlie$V1
charlie$load_diff_percent <- round(charlie$load_diff / charlie$loads * 100,0)
print(charlie)
```

 need to check that the output I generate from the scenarios is reasonable with respect to the original data which was used to calibrate the CASM model.
 The first check is to ensure that allthe points and distances are the same.
 The second check is to compare the leach rates for the different sumanagement zone- land use - LUC areas and identify those that have changed. Make sure that those that have changed are the correct ones.
 Check the amount that they have changed by.
 
```{r}

```
 
